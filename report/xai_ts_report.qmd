---
title: "XAI on Time Series Forecasting"
author: "Marco Zanotti"
date: "2023-11-20"
format: 
  html:
    embed-resources: true
    smooth-scroll: true
    theme: darkly
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 3
---

```{r logo, echo=FALSE}
htmltools::img(
	src = knitr::image_uri("img/logo-giallo.png"), 
  alt = 'logo', 
	style = 'position:absolute; top:0; right:0; padding:10px;'
)
```

```{r options, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE, 
  warning = FALSE
)

n_cores <- parallel::detectCores()
future::plan("multisession", workers = n_cores)
```


# Setup 

```{r installs, eval=FALSE}
install.packages("tidyverse")
install.packages("timetk")
install.packages("tidymodels")
install.packages("modeltime")
install.packages("modeltime.h2o")
install.packages("h2o")
install.packages("DALEX")
install.packages("DALEXtra")
install.packages("lime")
```

```{r load}
library(tidyverse)
library(timetk)
library(tidymodels)
library(modeltime)
library(modeltime.h2o)
library(h2o)
library(DALEX)
library(DALEXtra)
library(lime)
```



# Data 

The data used is an **hourly time series** from the **M4 competition**. 
The M4 competition is a large scale forecasting competition organized by 
Spyros Makridakis and his team. The competition was held in 2018 and the data 
is available on the [M4 competition website](https://www.m4.unic.ac.cy/the-dataset/). 
A sample of the data is available in the `data` folder of this repository.


## Import & Visualization

```{r import, fig.align='center'}
m4 <- read_csv("../data/m4.csv")

freq <- "Hourly"
ts_id <- "H413"
m4_ts <- m4 |> 
	filter(period == freq & id == ts_id) |> 
	select(-id, -type, -period)

m4_ts |> 
	plot_time_series(
		.date_var = date, .value = value, 
		.smooth = FALSE, .title = ts_id, .interactive = FALSE
	)
```


## Feature Engineering

In order to be able to use data in a machine learning model, we need to 
create features that can be used as predictors. This step is extremely 
relevant in time series data, since we need to create **features that are able to
capture the time dynamics of the data**. There exists a number of possible 
features that can be created; here I am using **lags, rolling features, calendar 
features and fourier series**, obtaining a total of **24 features**. 

```{r feateng}
horizon <- 48 # the forecast horizon, 2 days
lag_period <- 48 # 48h
rolling_periods <- c(12, 24) # 12h, 24h

m4_ts_prep <- m4_ts |> 
	# standardization
	mutate(value = standardize_vec(value)) |>  
	# add lags
	tk_augment_lags(value, .lags = lag_period) |>
	# add rolling features
	tk_augment_slidify(
		value_lag48, mean, .period = rolling_periods, .align = "center", .partial = TRUE
	) |>
	rename_with(~ str_remove_all(., "(value_)|(lag48_)")) |> 
	rename_with(~ str_remove_all(., "_")) |> 
	# calendar features
	tk_augment_timeseries_signature(date) |> 
	select(-matches("(diff)|(iso)|(xts)|(lbl)|(year)|(half)|(quarter)|(minute)|(second)|(hour12)|(qday)|(yday)")) |> 
	mutate(
		index.num = standardize_vec(index.num),
		am.pm = as.factor(am.pm),
	) |> 
	# fourier series
	tk_augment_fourier(date, .periods = rolling_periods, .K = 2) |> 
	rename_with(~ str_remove_all(., "date_")) |> 
	# drop na
	drop_na()
glimpse(m4_ts_prep)	
```


## Train & Test Sets

The data is then split considering a **test set of 2 days** to evaluate the 
models' performances. 

```{r split, fig.align='center'}
splits <- m4_ts_prep |> 
	time_series_split(assess = horizon, cumulative = TRUE)
splits |>
	tk_time_series_cv_plan() |>
	plot_time_series_cv_plan(
		date, value, .interactive = FALSE, .title = "Train-Test Split"
	)
```



# Modelling

ML models are estimated using an **AutoML approach**, which allows to 
efficiently estimate a large number of models and select the best one. The 
AutoML approach is performed using [**h2o**](https://h2o.ai/).


## H2O Setup

To use h2o, we need to initialize the Java Virtual Machine.  

```{r h2o init, include=FALSE}
Sys.setenv(JAVA_HOME = "/usr/lib/jvm/jdk-17/") # set java home path
h2o.init() # initialize h2o
h2o.no_progress() # disable progress bars
```

```{r h2o init 2, eval=FALSE}
Sys.setenv(JAVA_HOME = "/usr/lib/jvm/jdk-17/") # set java home path
h2o.init() # initialize h2o
h2o.no_progress() # disable progress bars
```

then, the data needs to be converted to h2o format  

```{r h2o data}
train_h2o <- as.h2o(training(splits)) # convert train data to h2o
test_h2o <- as.h2o(testing(splits)) # convert test data to h2o
```

and we need to specify the target variable and the predictors.  

```{r h2o vars}
target <- "value"
x_vars <- setdiff(names(train_h2o), c(target, "date"))
```


## AutoML

**h2o** automatically estimates and tests **6 different ML algorithms**:  

- DRF (This includes both the Distributed Random Forest (DRF) and
Extremely Randomized Trees (XRT) models)  

- GLM (Generalized Linear Model with regularization)  

- XGBoost (XGBoost GBM)  

- GBM (H2O GBM)  

- DeepLearning (Fully-connected multi-layer artificial neural network)  

- StackedEnsemble (Stacked Ensembles, includes an ensemble of all the
base models and ensembles using subsets of the base models)  

All this models are estimated using a 5-fold cross validation (since the data
in not too large) and the best model is selected based on the RMSE metric.  

```{r h2o automl 1, include=FALSE}
model_h2o_automl <- h2o.automl(
	y = target, x = x_vars,
	training_frame = train_h2o,
	max_runtime_secs = 10,
	max_runtime_secs_per_model = 10,
	max_models = 50,
	nfolds = 5,
	sort_metric = "rmse",
	verbosity = NULL,
	seed = 123
)
```

```{r h2o automl 2, eval=FALSE}
model_h2o_automl <- h2o.automl(
	y = target, x = x_vars,
	training_frame = train_h2o,
	max_runtime_secs = 120,
	max_runtime_secs_per_model = 30,
	max_models = 50,
	nfolds = 5,
	sort_metric = "rmse",
	verbosity = NULL,
	seed = 123
)
```


## Model Evaluation

The best model is extracted from the AutoML object and evaluated both on the 
train and the test sets.   

```{r h2o leaderboard}
leader_board <- h2o.get_leaderboard(model_h2o_automl)
head(leader_board, 10)[, 1:2]
tail(leader_board, 10)[, 1:2]
```

```{r h2o best model}
h2o_best <- h2o.get_best_model(model_h2o_automl)
h2o_best
```

The performance of the best model on the train set is given by  

```{r h2o train performance}
h2o.performance(h2o_best, train_h2o)
```
![](img/ts_fit.png){align=center}	 

while the performance on the test set is  

```{r h2o test performance}
h2o.performance(h2o_best, test_h2o)
```
![](img/ts_pred.png){align=center}  



# XAI

In the context of time series forecasting, being able to understand the model's
predictions is of paramount importance for 3 main reasons:  

1. **Trust**: to be able to trust the model's predictions, especially 
when it is used to make important future decisions.  

2. **Improvement**: to be able to understand the model's weaknesses to improve it.  

3. **Combine**: usually model's predictions are combined with human judgement, 
so understanding what causes such predictions is crucial for business experts to
adjust their forecasts.  

Given the fact that AutoML models are black-boxes, we would like to better 
understand why the model is predicting what it is predicting. To do so, it is 
possible to use the **DALEX** package, from [Dr. Why AI](https://dalex.drwhy.ai/), 
which allows to easily adopt several model explainability techniques, from 
global to local ones, to obtain an in-depth analysis of the model's results.   


## Explainer

Using **DALEX** is almost straightforward since it is model agnostic and it 
interfaces with most of ML and DL frameworks (scikit-learn, keras, tensorflow,
h2o, tidymodels, mlr) through the **DALEXtra** extension.  
First, it is necessary to create a **DALEX explainer**  

```{r explainer, eval=FALSE}
explainer_h2o_automl <- explain_h2o(
	model = h2o_best, 
	data = select(testing(splits), -date, -value),
	y = testing(splits)$value,
	label = "h2o automl",
	type = "regression",
	colorize = FALSE
)
```


## Global Explanations

Then, it is possible to use the explainer to obtain dataset-level or 
**global explanations** of the model. This analysis helps to understand how the 
model predictions perform overall, for an entire set of observations. Hence, 
assuming that the observations form a representative sample from a general 
population, global explainers can provide information about the quality of 
predictions for the population.  

### Feature Importance

First, we can inspect the **feature importance** of the model, that is the 
relevance of each feature in the model's predictions.  

```{r feature importance, eval=FALSE}
fe_h2o_automl <- model_parts(explainer_h2o_automl)
plot(fe_h2o_automl)
```
![](img/xai_global_featimp.png){align=center}  

### Partial Dependence

Then, we can inspect the **partial dependence** of the model, that shows how 
does the expected value of model prediction behave as a function of a selected 
explanatory variable. Hence, it is a simple way to summarize the effect of a 
particular explanatory variable on the dependent variable. Moreover, this tool 
is very useful to understand the model performance at boundaries (since models 
may have strange behaviours at the boundaries of the possible range of a 
variable).  

```{r partial dependence, eval=FALSE}
pdp_h2o_automl <- model_profile(
	explainer_h2o_automl, 
	variable = c("hour", "wday"),  
	type = "partial"
)
plot(pdp_h2o_automl)
```
![](img/xai_global_partdep.png){align=center}  

### Global Results

As shown by the **Feature Importance** plot, the most important features are 
those related to **daily seasonality**, while weekly and monthly seasonality seem 
to be irrelevant. The **day of the week** is also of great importance.  

The PD for the **hour** and the **day of the week** variables reveals a clear 
intra-day pattern, with higher predictions during the night, and a downward
trending effect within the week (where wday = 1 means Sunday).  


## Local Explanations

It is possible to use the explainer to obtain also instance-level or 
**local explanations**, helping understand how a model yields a prediction for 
a particular single observation. Indeed, in this case it is necessary to specify 
an observation to be explained. In the context of time series forecasting, one 
may explore test observations to inspect how future values are produced by the 
model.  
Here, in particular, I focused on test observations where the model is failing, 
that is  

- in the **afternoon** of the **23rd of July 2017** and  

- in the **first hours** of the **23th of July 2017** and **24th of July 2017**,  

possibly suggesting that the model is not completely capturing intra-day dynamics.  
(The results for the first hours of the 23 and 24 of July are almost the same)  

```{r test obs, eval=FALSE}
new_value_2h <- testing(splits) |> filter(date == ymd_hms("2017-07-23 02:00:00"))
new_value_4h <- testing(splits) |> filter(date == ymd_hms("2017-07-24 04:00:00"))
new_value_15h <- testing(splits) |> filter(date == ymd_hms("2017-07-23 15:00:00"))
```

### Break Down

In the context of local explainability, one of the main questions that one may 
want to answer is "which variables contribute to this result the most?", and 
several different approaches have been proposed to address this issue. 
The **break-down** method shows how the contributions attributed to individual 
explanatory variables change the mean model’s prediction to yield the actual 
prediction for a particular single instance.  

```{r break down, eval=FALSE}
bd_h2o_automl <- predict_parts(
	explainer_h2o_automl,
	new_observation = new_value_xh,
	type = "break_down"
)
plot(bd_h2o_automl)
```
![Observation: 2017-07-23 15:00:00](img/xai_local_breakdown_15h.png){align=center}  

![Observation: 2017-07-24 04:00:00](img/xai_local_breakdown_4h.png){align=center}  

### SHAP

**SHapley Additive exPlanations** (SHAP) is another method used to understand what 
variables contributed the most to a particular prediction and it is based on 
Shapley values from game theory.  


```{r shap, eval=FALSE}
shap_h2o_automl <- predict_parts(
	explainer_h2o_automl,
	new_observation = new_value_xh,
	type = "shap"
)
plot(shap_h2o_automl)
```
![Observation: 2017-07-23 15:00:00](img/xai_local_shap_15h.png){align=center}  

![Observation: 2017-07-24 04:00:00](img/xai_local_shap_4h.png){align=center}  

### Lime

Break-down (BD) plots and Shapley values are most suitable for models with a 
small or moderate number of explanatory variables. None of those approaches is 
well-suited for models with a very large number of explanatory variables, 
because they usually determine non-zero attributions for all variables in the 
model. However, when the number of explanatory variables is huge, **sparse 
explanations** with a small number of variables may offer a useful alternative. 
The most popular of such sparse explainers is the **Local Interpretable 
Model-agnostic Explanations** (LIME) method. The key idea behind it is to 
locally approximate a black-box model by a simpler and easy to interpret linear 
regression model.  

```{r lime, eval=FALSE}
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer

lime_h2o_automl <- predict_surrogate(
	explainer = explainer_h2o_automl, 
	new_observation = new_value_xh, 
	n_features = 10, 
	n_permutations = 1000,
	type = "lime"
)
plot(lime_h2o_automl)
```
![Observation: 2017-07-23 15:00:00](img/xai_local_lime_15h.png){align=center}  

![Observation: 2017-07-24 04:00:00](img/xai_local_lime_4h.png){align=center}  

However, the LIME method is not well suited for time series data, because there 
are several important limitations in case of tabular data. In particular, there 
have been various proposals for finding interpretable representations for 
continuous and categorical explanatory variables but the issue has not been 
solved yet.  

### Ceteris Paribus

The previous local explainability methods quantified the importance of 
explanatory variables in the context of a single-instance prediction and their 
application yields a decomposition of the prediction into components that can 
be attributed to particular variables.  
**Ceteris Paribus**, instead, is a method that evaluates the effect of a selected 
explanatory variable in terms of changes of a model’s prediction induced by 
changes in the variable’s values. The method is based on the ceteris paribus 
principle, that is it examines the influence of an explanatory variable by 
assuming that the values of all other variables do not change (i.e. the 
dependence of the conditional expectation of the dependent variable on the 
values of the particular explanatory variable).    

```{r ceteris paribus, eval=FALSE}
cetp_h2o_automl <- predict_profile(
	explainer = explainer_h2o_automl, 
	new_observation = new_value_xh,
	type = "ceteris_paribus",
	variables = c("hour", "wday")
)
plot(cetp_h2o_automl, variables = c("hour", "wday"))
```
![Observation: 2017-07-23 15:00:00](img/xai_local_cetpar_15h.png){align=center}   

![Observation: 2017-07-24 04:00:00](img/xai_local_cetpar_4h.png){align=center}  

### Stability Analysis

It may happen that, despite the fact that the predictive performance of a model 
is satisfactory overall, the model’s predictions for some observations are 
drastically worse. For example, a model developed to forecast product demand 
may perform well during spring and may not in autumn. In this case, it is 
worthwhile to check how does the model behave locally for observations similar 
to the instance of interest and the **Local-stability** analysis allows to 
assess the (local) stability of predictions around the observation of interest. 
It is based first on the identification of a set of neighbors for the observation 
of interest and then on the comparison of ceteris paribus profiles for selected 
explanatory variables.  

```{r stability, eval=FALSE}
preddiag_h2o_automl <- predict_diagnostics(
	explainer = explainer_h2o_automl, 
	new_observation = new_value_xh,
	variables = c("hour", "wday"),
	neighbors = 50,
  distance = gower::gower_dist
)
plot(preddiag_h2o_automl, variables = c("hour", "wday"))
```
![Observation: 2017-07-23 15:00:00](img/xai_local_stab_15h.png){align=center}  

![Observation: 2017-07-24 04:00:00](img/xai_local_stab_4h.png){align=center}  

### Local Results

The Break Down, Shapley values and LIME yield all very similar results. However,
the **Break Down** method is the only one that allows to quantify the contribution
of the variables to the prediction. Hence, it is the most useful for the purpose
of explaining how a prediction has been produced by the models within a business
forecasting process. For instance, in the case of the early morning prediction, 
the hour, the daily seasonality but also the week are all very relevant.  

Finally, the **Stability Analysis** shows that the profiles are not relatively 
close to each other, suggesting some "instability" of predictions.  
Moreover, there are very positive residuals in the morning, suggesting that the
model is on average underestimating the demand in the morning, while there are 
more negative residuals in the afternoon and evening, implying that the model 
is on average overestimating the demand in that part of the day. This information
is extremely useful for the judgemental forecasting adjustment business experts 
may adopt to improve business results.  


----

Finally, remember to shut down the h2o cluster.  

```{r h2o shutdown}
h2o.shutdown(prompt = FALSE)
```

